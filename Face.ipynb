{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d41269-a403-4b27-90df-a63b9cd5d884",
      "metadata": {
        "tags": [],
        "id": "67d41269-a403-4b27-90df-a63b9cd5d884"
      },
      "outputs": [],
      "source": [
        "!pip install pretrainedmodels torchsummary opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f9435f-b2fe-4bc9-bbec-dac87f4845f8",
      "metadata": {
        "tags": [],
        "id": "b5f9435f-b2fe-4bc9-bbec-dac87f4845f8"
      },
      "outputs": [],
      "source": [
        "import dlib\n",
        "dlib.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59b5055-1483-4aa7-b54e-cc41082fdf5d",
      "metadata": {
        "tags": [],
        "id": "f59b5055-1483-4aa7-b54e-cc41082fdf5d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torchvision import transforms\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.nn import init\n",
        "import os\n",
        "import argparse\n",
        "import pretrainedmodels\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from keras.models import load_model\n",
        "from os.path import join\n",
        "import cv2\n",
        "import dlib\n",
        "from PIL import Image as pil_image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c411237-7899-4f6f-83ae-68c0c448112b",
      "metadata": {
        "tags": [],
        "id": "1c411237-7899-4f6f-83ae-68c0c448112b"
      },
      "outputs": [],
      "source": [
        "xception_default_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94f5129-ffe9-4470-bfc7-c7cfdf904679",
      "metadata": {
        "tags": [],
        "id": "b94f5129-ffe9-4470-bfc7-c7cfdf904679"
      },
      "outputs": [],
      "source": [
        "pretrained_settings = {\n",
        "    'xception': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 299, 299],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.5, 0.5, 0.5],\n",
        "            'std': [0.5, 0.5, 0.5],\n",
        "            'num_classes': 1000,\n",
        "            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2efb6367-64a4-41b9-be09-9b4fffb59e6a",
      "metadata": {
        "tags": [],
        "id": "2efb6367-64a4-41b9-be09-9b4fffb59e6a"
      },
      "outputs": [],
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab561363-cf85-4711-bbb5-b593651753d4",
      "metadata": {
        "tags": [],
        "id": "ab561363-cf85-4711-bbb5-b593651753d4"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        if out_filters != in_filters or strides!=1:\n",
        "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
        "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
        "        else:\n",
        "            self.skip=None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        rep=[]\n",
        "\n",
        "        filters=in_filters\n",
        "        if grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "            filters = out_filters\n",
        "\n",
        "        for i in range(reps-1):\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "        if not grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "\n",
        "        if not start_with_relu:\n",
        "            rep = rep[1:]\n",
        "        else:\n",
        "            rep[0] = nn.ReLU(inplace=False)\n",
        "\n",
        "        if strides != 1:\n",
        "            rep.append(nn.MaxPool2d(3,strides,1))\n",
        "        self.rep = nn.Sequential(*rep)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.rep(inp)\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(inp)\n",
        "            skip = self.skipbn(skip)\n",
        "        else:\n",
        "            skip = inp\n",
        "\n",
        "        x+=skip\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217a54f6-c973-4a7a-bf68-83ccdc55def3",
      "metadata": {
        "tags": [],
        "id": "217a54f6-c973-4a7a-bf68-83ccdc55def3"
      },
      "outputs": [],
      "source": [
        "class Xception(nn.Module):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset, as specified in\n",
        "    https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        #do relu here\n",
        "\n",
        "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
        "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
        "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        #do relu here\n",
        "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def features(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.block11(x)\n",
        "        x = self.block12(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        return x\n",
        "\n",
        "    def logits(self, features):\n",
        "        x = self.relu(features)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.last_linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.features(input)\n",
        "        x = self.logits(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2d3450-f9ff-4b8e-bcff-0592376644f7",
      "metadata": {
        "tags": [],
        "id": "9a2d3450-f9ff-4b8e-bcff-0592376644f7"
      },
      "outputs": [],
      "source": [
        "def xception(num_classes=1000, pretrained='imagenet'):\n",
        "    model = Xception(num_classes=num_classes)\n",
        "    if pretrained:\n",
        "        settings = pretrained_settings['xception'][pretrained]\n",
        "        assert num_classes == settings['num_classes'], \\\n",
        "            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n",
        "\n",
        "        model = Xception(num_classes=num_classes)\n",
        "        model.load_state_dict(model_zoo.load_url(settings['url']))\n",
        "\n",
        "        model.input_space = settings['input_space']\n",
        "        model.input_size = settings['input_size']\n",
        "        model.input_range = settings['input_range']\n",
        "        model.mean = settings['mean']\n",
        "        model.std = settings['std']\n",
        "\n",
        "    # TODO: ugly\n",
        "    model.last_linear = model.fc\n",
        "    del model.fc\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cc9ec0-4c8a-4ec0-8aae-e8d66ceb5ad3",
      "metadata": {
        "tags": [],
        "id": "33cc9ec0-4c8a-4ec0-8aae-e8d66ceb5ad3"
      },
      "outputs": [],
      "source": [
        "def return_pytorch04_xception(pretrained=True):\n",
        "    # Raises warning \"src not broadcastable to dst\" but thats fine\n",
        "    model = xception(pretrained=False)\n",
        "    if pretrained:\n",
        "        # Load model in torch 0.4+\n",
        "        model.fc = model.last_linear\n",
        "        del model.last_linear\n",
        "        # path = os.path.join(os.path.dirname(__file__), \"sagemaker-studiolab-notebooks/xception-b5690688.pth\")\n",
        "        path = \"xception-b5690688.pth\"\n",
        "        state_dict = torch.load(path)\n",
        "        for name, weights in state_dict.items():\n",
        "            if 'pointwise' in name:\n",
        "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.last_linear = model.fc\n",
        "        del model.fc\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1027e344-0680-4038-a950-495dec00d856",
      "metadata": {
        "tags": [],
        "id": "1027e344-0680-4038-a950-495dec00d856"
      },
      "outputs": [],
      "source": [
        "class TransferModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple transfer learning model that takes an imagenet pretrained model with\n",
        "    a fc layer as base model and retrains a new fc layer for num_out_classes\n",
        "    \"\"\"\n",
        "    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.modelchoice = modelchoice\n",
        "        if modelchoice == 'xception':\n",
        "            self.model = return_pytorch04_xception()\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.last_linear.in_features\n",
        "            if not dropout:\n",
        "                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                print('Using dropout', dropout)\n",
        "                self.model.last_linear = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n",
        "            if modelchoice == 'resnet50':\n",
        "                self.model = torchvision.models.resnet50(pretrained=True)\n",
        "            if modelchoice == 'resnet18':\n",
        "                self.model = torchvision.models.resnet18(pretrained=True)\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.fc.in_features\n",
        "            if not dropout:\n",
        "                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                self.model.fc = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        elif modelchoice == 'densenet121':\n",
        "            self.model = torchvision.models.densenet121(pretrained=True)\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.classifier.in_features\n",
        "            if not dropout:\n",
        "                self.model.classifier = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                self.model.classifier = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        else:\n",
        "            raise Exception('Choose valid model, e.g. resnet50')\n",
        "\n",
        "    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n",
        "        \"\"\"\n",
        "        Freezes all layers below a specific layer and sets the following layers\n",
        "        to true if boolean else only the fully connected final layer\n",
        "        :param boolean:\n",
        "        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Stage-1: freeze all the layers\n",
        "        if layername is None:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = True\n",
        "                return\n",
        "        else:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "        if boolean:\n",
        "            # Make all layers following the layername layer trainable\n",
        "            ct = []\n",
        "            found = False\n",
        "            for name, child in self.model.named_children():\n",
        "                if layername in ct:\n",
        "                    found = True\n",
        "                    for params in child.parameters():\n",
        "                        params.requires_grad = True\n",
        "                ct.append(name)\n",
        "            if not found:\n",
        "                raise Exception('Layer not found, cant finetune!'.format(\n",
        "                    layername))\n",
        "        else:\n",
        "            if self.modelchoice == 'xception':\n",
        "                # Make fc trainable\n",
        "                for param in self.model.last_linear.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            elif self.modelchoice == 'resnet50' or self.modelchoice == 'resnet18':\n",
        "                # Make fc trainable\n",
        "                for param in self.model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            elif self.modelchoice == 'densenet121':\n",
        "                # Make fc trainable\n",
        "                for param in self.model.classifier.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            else:\n",
        "                # Make fc trainable\n",
        "                for param in self.model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a56afe-74c9-4e94-b51b-4fe33e766e4c",
      "metadata": {
        "tags": [],
        "id": "40a56afe-74c9-4e94-b51b-4fe33e766e4c"
      },
      "outputs": [],
      "source": [
        "def model_selection(modelname, num_out_classes,\n",
        "                    dropout=None):\n",
        "    \"\"\"\n",
        "    :param modelname:\n",
        "    :return: model, image size, pretraining<yes/no>, input_list\n",
        "    \"\"\"\n",
        "    if modelname == 'xception':\n",
        "        return TransferModel(modelchoice='xception',\n",
        "                             num_out_classes=num_out_classes), 299, \\\n",
        "               True, ['image'], None\n",
        "    elif modelname == 'resnet18':\n",
        "        return TransferModel(modelchoice='resnet18', dropout=dropout,\n",
        "                             num_out_classes=num_out_classes), \\\n",
        "               224, True, ['image'], None\n",
        "    elif modelname == 'densenet121':\n",
        "        return TransferModel(modelchoice='densenet121', dropout=dropout,\n",
        "                             num_out_classes=num_out_classes), \\\n",
        "               224, True, ['image'], None\n",
        "    else:\n",
        "        raise NotImplementedError(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22edac5c-794a-499e-9c47-263d50e6c864",
      "metadata": {
        "tags": [],
        "id": "22edac5c-794a-499e-9c47-263d50e6c864"
      },
      "outputs": [],
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47aee039-35d0-4059-b4d5-7ec329f7ee5a",
      "metadata": {
        "tags": [],
        "id": "47aee039-35d0-4059-b4d5-7ec329f7ee5a"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image, cuda=True):\n",
        "    \"\"\"\n",
        "    Preprocesses the image such that it can be fed into our network.\n",
        "    During this process we envoke PIL to cast it into a PIL image.\n",
        "\n",
        "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
        "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
        "    necessarily casted to cuda\n",
        "    \"\"\"\n",
        "    # Revert from BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Preprocess using the preprocessing function used during training and\n",
        "    # casting it to PIL image\n",
        "    preprocess = xception_default_data_transforms['test']\n",
        "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
        "    # Add first dimension as the network expects a batch\n",
        "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
        "    if cuda:\n",
        "        preprocessed_image = preprocessed_image.cuda()\n",
        "    return preprocessed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04d964e-8182-4f8c-9066-a6281194b2ba",
      "metadata": {
        "tags": [],
        "id": "a04d964e-8182-4f8c-9066-a6281194b2ba"
      },
      "outputs": [],
      "source": [
        "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),\n",
        "                       cuda=True):\n",
        "    \"\"\"\n",
        "    Predicts the label of an input image. Preprocesses the input image and\n",
        "    casts it to cuda if required\n",
        "\n",
        "    :param image: numpy image\n",
        "    :param model: torch model with linear layer at the end\n",
        "    :param post_function: e.g., softmax\n",
        "    :param cuda: enables cuda, must be the same parameter as the model\n",
        "    :return: prediction (1 = fake, 0 = real)\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    preprocessed_image = preprocess_image(image, cuda)\n",
        "\n",
        "    # Model prediction\n",
        "    output = model(preprocessed_image)\n",
        "    output = post_function(output)\n",
        "\n",
        "    # Cast to desired\n",
        "    _, prediction = torch.max(output, 1)    # argmax\n",
        "    prediction = float(prediction.cpu().numpy())\n",
        "\n",
        "    return int(prediction), output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb3e296-c074-400f-9f41-f5c0bd90c482",
      "metadata": {
        "tags": [],
        "id": "6eb3e296-c074-400f-9f41-f5c0bd90c482"
      },
      "outputs": [],
      "source": [
        "def test_full_image_network(video_path, model_path=None,\n",
        "                            start_frame=0, end_frame=None, cuda=True):\n",
        "    \"\"\"\n",
        "    Reads a video and evaluates a subset of frames with the a detection network\n",
        "    that takes in a full frame. Outputs are only given if a face is present\n",
        "    and the face is highlighted using dlib.\n",
        "    :param video_path: path to video file\n",
        "    :param model_path: path to model file (should expect the full sized image)\n",
        "    :param start_frame: first frame to evaluate\n",
        "    :param end_frame: last frame to evaluate\n",
        "    :param cuda: enable cuda\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('Starting: {}'.format(video_path))\n",
        "\n",
        "    # Read and write\n",
        "    reader = cv2.VideoCapture(video_path)\n",
        "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Load model\n",
        "    model1, *_ = model_selection(modelname='densenet121', num_out_classes=2)\n",
        "    if model_path is not None:\n",
        "        model1 = torch.load(model_path)\n",
        "    if cuda:\n",
        "        model1 = model1.cuda()\n",
        "\n",
        "    model2, *_ = model_selection(modelname='resnet18', num_out_classes=2)\n",
        "    if model_path is not None:\n",
        "        model2 = torch.load(model_path)\n",
        "    if cuda:\n",
        "        model2 = model2.cuda()\n",
        "\n",
        "    model3, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
        "    if model_path is not None:\n",
        "        model3 = torch.load(model_path)\n",
        "    if cuda:\n",
        "        model3 = model3.cuda()\n",
        "\n",
        "    # Frame numbers and length of output video\n",
        "    frame_num = 0\n",
        "    assert start_frame < num_frames - 1\n",
        "    end_frame = end_frame if end_frame else num_frames\n",
        "\n",
        "    predictions1 = []\n",
        "    predictions2 = []\n",
        "    predictions3 = []\n",
        "\n",
        "    while reader.isOpened():\n",
        "        _, image = reader.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num < start_frame:\n",
        "            continue\n",
        "\n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # 2. Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            # For now only take biggest face\n",
        "            face = faces[0]\n",
        "\n",
        "            # --- Prediction ---------------------------------------------------\n",
        "            # Face crop with dlib and bounding box scale enlargement\n",
        "            x, y, size = get_boundingbox(face, width, height)\n",
        "            cropped_face = image[y:y+size, x:x+size]\n",
        "\n",
        "            # Actual prediction using our model\n",
        "            # Model 1 : densenet121\n",
        "            prediction, output1 = predict_with_model(cropped_face, model1,\n",
        "                                                    cuda=cuda)\n",
        "            predictions1.append(prediction)\n",
        "\n",
        "            # Model 2 : resnet18\n",
        "            prediction2, output2 = predict_with_model(cropped_face, model2,\n",
        "                                                    cuda=cuda)\n",
        "            predictions2.append(prediction2)\n",
        "\n",
        "            # Model 3 : xception\n",
        "            prediction3, output3 = predict_with_model(cropped_face, model3,\n",
        "                                                    cuda=cuda)\n",
        "            predictions3.append(prediction3)\n",
        "            # ------------------------------------------------------------------\n",
        "\n",
        "        if frame_num >= end_frame:\n",
        "            break\n",
        "\n",
        "    # return predictions but wich number appear the most\n",
        "    model1_prediction = max(set(predictions1), key=predictions1.count)\n",
        "    model2_prediction = max(set(predictions2), key=predictions2.count)\n",
        "    model3_prediction = max(set(predictions3), key=predictions3.count)\n",
        "    print(model1_prediction, model2_prediction, model3_prediction)\n",
        "    final = [model1_prediction, model2_prediction, model3_prediction]\n",
        "\n",
        "    return max(set(final), key=final.count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a1d647-9fc7-4d5d-9ec9-cc826f91f17c",
      "metadata": {
        "tags": [],
        "id": "37a1d647-9fc7-4d5d-9ec9-cc826f91f17c"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eRuU4OVJxJBF"
      },
      "id": "eRuU4OVJxJBF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "already = {'df2_fake_id0_id16_0007.mp4': '[1, 1, 0, 1]', 'df2_fake_id30_id6_0008.mp4': '[1, 1, 0, 1]', 'df2_fake_id0_id20_0007.mp4': '[0, 1, 1, 1]', 'df2_fake_id31_id20_0009.mp4': '[0, 1, 0, 0]', 'df2_fake_id0_id3_0007.mp4': '[0, 1, 0, 0]', 'df2_fake_id31_id29_0002.mp4': '[0, 0, 0, 0]', 'df2_fake_id10_id12_0007.mp4': '[0, 0, 1, 0]', 'df2_fake_id35_id16_0007.mp4': '[0, 0, 0, 0]', 'df2_fake_id16_id3_0006.mp4': '[0, 1, 1, 1]', 'df2_fake_id35_id17_0008.mp4': '[1, 0, 1, 1]', 'df2_fake_id16_id9_0003.mp4': '[1, 1, 0, 1]', 'df2_fake_id35_id20_0003.mp4': '[0, 0, 0, 0]', 'df2_fake_id17_id26_0003.mp4': '[0, 1, 0, 0]', 'df2_fake_id35_id20_0007.mp4': '[1, 0, 1, 1]', 'df2_fake_id17_id28_0005.mp4': '[0, 1, 0, 0]', 'df2_fake_id35_id33_0004.mp4': '[1, 0, 0, 0]', 'df2_fake_id17_id3_0005.mp4': '[1, 1, 0, 1]', 'df2_fake_id35_id6_0000.mp4': '[1, 1, 1, 1]', 'df2_fake_id1_id31_0000.mp4': '[0, 0, 1, 0]', 'df2_fake_id35_id9_0009.mp4': '[0, 0, 0, 0]', 'df2_fake_id1_id3_0005.mp4': '[1, 1, 0, 1]', 'df2_fake_id37_id17_0000.mp4': '[0, 1, 1, 1]', 'df2_fake_id20_id16_0005.mp4': '[1, 1, 1, 1]', 'df2_fake_id37_id31_0006.mp4': '[1, 0, 1, 1]', 'df2_fake_id20_id1_0006.mp4': '[1, 0, 1, 1]', 'df2_fake_id38_id28_0002.mp4': '[0, 1, 0, 0]', 'df2_fake_id20_id2_0008.mp4': '[0, 1, 1, 1]', 'df2_fake_id38_id28_0003.mp4': '[0, 0, 0, 0]', 'df2_fake_id20_id31_0002.mp4': '[1, 1, 0, 1]', 'df2_fake_id38_id30_0003.mp4': '[1, 1, 1, 1]', 'df2_fake_id20_id32_0005.mp4': '[0, 1, 1, 1]', 'df2_fake_id3_id0_0008.mp4': '[0, 0, 0, 0]', 'df2_fake_id21_id23_0002.mp4': '[1, 1, 1, 1]', 'df2_fake_id3_id30_0007.mp4': '[0, 1, 1, 1]', 'df2_fake_id21_id27_0002.mp4': '[1, 0, 0, 0]', 'df2_fake_id3_id31_0000.mp4': '[0, 0, 1, 0]', 'df2_fake_id22_id28_0005.mp4': '[0, 1, 1, 1]', 'df2_fake_id40_id44_0002.mp4': '[1, 1, 0, 1]', 'df2_fake_id23_id20_0009.mp4': '[1, 1, 0, 1]', 'df2_fake_id41_id40_0004.mp4': '[1, 0, 1, 1]', 'df2_fake_id23_id25_0006.mp4': '[1, 1, 0, 1]', 'df2_fake_id41_id40_0006.mp4': '[1, 0, 1, 1]', 'df2_fake_id23_id35_0009.mp4': '[0, 1, 1, 1]', 'df2_fake_id41_id46_0009.mp4': '[1, 1, 0, 1]', 'df2_fake_id23_id38_0003.mp4': '[0, 1, 0, 0]', 'df2_fake_id41_id48_0003.mp4': '[1, 0, 1, 1]', 'df2_fake_id23_id3_0009.mp4': '[1, 0, 1, 1]', 'df2_fake_id46_id42_0006.mp4': '[0, 1, 0, 0]', 'df2_fake_id24_id25_0002.mp4': '[1, 0, 1, 1]', 'df2_fake_id46_id47_0002.mp4': '[0, 0, 0, 0]', 'df2_fake_id24_id26_0003.mp4': '[1, 1, 1, 1]', 'df2_fake_id48_id46_0005.mp4': '[0, 1, 0, 0]', 'df2_fake_id26_id1_0002.mp4': '[1, 1, 0, 1]', 'df2_fake_id48_id47_0003.mp4': '[0, 0, 0, 0]', 'df2_fake_id26_id38_0004.mp4': '[0, 1, 1, 1]', 'df2_fake_id49_id51_0006.mp4': '[1, 0, 0, 0]', 'df2_fake_id28_id22_0005.mp4': '[0, 1, 1, 1]', 'df2_fake_id49_id53_0008.mp4': '[0, 0, 1, 0]', 'df2_fake_id28_id38_0007.mp4': '[0, 1, 0, 0]', 'df2_fake_id4_id20_0005.mp4': '[1, 0, 0, 0]', 'df2_fake_id28_id9_0003.mp4': '[1, 0, 0, 0]', 'df2_fake_id4_id26_0007.mp4': '[0, 1, 1, 1]', 'df2_fake_id28_id9_0004.mp4': '[0, 1, 1, 1]', 'df2_fake_id4_id30_0001.mp4': '[0, 1, 1, 1]', 'df2_fake_id29_id28_0006.mp4': '[0, 1, 0, 0]', 'df2_fake_id4_id3_0003.mp4': '[1, 1, 1, 1]', 'df2_fake_id29_id34_0004.mp4': '[0, 1, 0, 0]', 'df2_fake_id4_id9_0006.mp4': '[1, 1, 1, 1]', 'df2_fake_id29_id38_0001.mp4': '[0, 1, 0, 0]', 'df2_fake_id50_id52_0001.mp4': '[0, 0, 0, 0]', 'df2_fake_id2_id35_0000.mp4': '[0, 1, 0, 0]', 'df2_fake_id51_id54_0007.mp4': '[0, 1, 1, 1]', 'df2_fake_id2_id35_0007.mp4': '[1, 0, 1, 1]', 'df2_fake_id52_id50_0007.mp4': '[0, 0, 0, 0]', 'df2_fake_id2_id3_0005.mp4': '[1, 0, 1, 1]', 'df2_fake_id52_id53_0006.mp4': '[0, 0, 1, 0]', 'df2_fake_id30_id20_0004.mp4': '[0, 0, 0, 0]', 'df2_fake_id53_id54_0006.mp4': '[0, 0, 0, 0]', 'df2_fake_id30_id23_0002.mp4': '[1, 0, 0, 0]', 'df2_fake_id53_id57_0007.mp4': '[1, 0, 0, 0]', 'df2_fake_id30_id33_0008.mp4': '[0, 0, 1, 0]', 'df2_fake_id31_id2_0005.mp4': '[0, 0, 1, 0]', 'df2_fake_id54_id49_0002.mp4': '[0, 0, 0, 0]', 'df2_fake_id31_id9_0004.mp4': '[1, 1, 1, 1]', 'df2_fake_id54_id55_0006.mp4': '[0, 0, 1, 0]', 'df2_fake_id33_id20_0005.mp4': '[1, 1, 0, 1]', 'df2_fake_id55_id53_0009.mp4': '[1, 1, 1, 1]', 'df2_fake_id33_id37_0000.mp4': '[1, 1, 0, 1]'}"
      ],
      "metadata": {
        "id": "NRAzQIpknkBg"
      },
      "id": "NRAzQIpknkBg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import time\n",
        "\n",
        "output_file_path = \"/content/drive/MyDrive/output22.txt\"  # Change the path as needed\n",
        "video_path = \"/content/drive/MyDrive/lucky2\"\n",
        "\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    if video_path.endswith('.mp4') or video_path.endswith('.avi'):\n",
        "        prediction = test_full_image_network(video_path=video_path)\n",
        "        output_file.write('Video: {}, Prediction: {}\\n'.format(video_path, prediction))\n",
        "    else:\n",
        "      output_file = open(output_file_path, 'w')\n",
        "      videos = os.listdir(video_path)\n",
        "      predictions = {}\n",
        "\n",
        "      for video in videos:\n",
        "          if video in already:\n",
        "            print(\"Already\")\n",
        "            continue\n",
        "          video_paths = join(video_path, video)\n",
        "          start_time = time.time()\n",
        "          prediction = test_full_image_network(video_path=video_paths, model_path=None)\n",
        "          print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "          predictions[video] = prediction\n",
        "          output_file.write('Video: {}, Prediction: {}\\n'.format(video, prediction))\n",
        "          print('Video: {}, Prediction: {}'.format(video, prediction))\n",
        "\n",
        "      output_file.close()\n",
        "\n",
        "      print(predictions)"
      ],
      "metadata": {
        "id": "VzZLlEbyFeBk"
      },
      "id": "VzZLlEbyFeBk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}